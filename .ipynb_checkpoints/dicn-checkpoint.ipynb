{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9801aa9a-2333-4085-b37a-c81a2e311b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my-env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from ne_level.model import SuperGATNet\n",
    "from ne_level.arguments import get_args\n",
    "from ne_level.data import getattr_d, get_dataset_or_loader\n",
    "from ne_level.layer import SuperGAT\n",
    "from ne_level.layer_cgat import CGATConv\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import utils\n",
    "from utils import data_preprocessing, get_dataset\n",
    "from model import GCNModelGumbel\n",
    "from evaluation import eva\n",
    "import community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40894b89-421c-4863-bd3b-dc6119898af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DICN(nn.Module):\n",
    "    def __init__(self, device, dropout, edge_num, embedding_vg, num_features, node_num, embedding_size, alpha,\n",
    "                 num_clusters, v=1):\n",
    "        super(DICN, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.v = v\n",
    "        self.node_num = node_num\n",
    "        self.gat = SuperGATNet(args=sgat_args, dataset_or_loader=train_d)\n",
    "\n",
    "        self.vGraph = GCNModelGumbel(node_num, embedding_vg, num_clusters, dropout, device)\n",
    "        # self.encode1=nn.Linear(num_clusters,num_clusters) # (7,7)\n",
    "        # self.dropout=nn.Dropout(0.5)\n",
    "        # self.encode3=nn.Linear(embedding_size,embedding_size)  # (128,128)\n",
    "\n",
    "        # cluster layer\n",
    "        self.cluster_layer = Parameter(torch.Tensor(num_clusters, embedding_size))  # （7,128）\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def pre_forward(self, dataset):\n",
    "        z = self.gat(dataset.x, dataset.edge_index,\n",
    "                     batch=getattr(dataset, \"batch\", None),\n",
    "                     attention_edge_index=getattr(dataset, \"train_edge_index\", None))\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, dataset, w, c, temp):\n",
    "\n",
    "        z = self.gat(dataset.x, dataset.edge_index,\n",
    "                     batch=getattr(dataset, \"batch\", None),\n",
    "                     attention_edge_index=getattr(dataset, \"train_edge_index\", None))\n",
    "        q = self.get_Q(z)\n",
    "\n",
    "        recon_c, q_vg, prior, node_embeddings, community_embeddings = self.vGraph(w, c, temp)\n",
    "\n",
    "        res = torch.zeros([args.node_num, args.n_clusters], dtype=torch.float32).to(device)\n",
    "        for idx, e in enumerate(args.train_edges):\n",
    "            res[e[0], :] += q_vg[idx, :]\n",
    "            res[e[1], :] += q_vg[idx, :]\n",
    "        from torch.nn.functional import normalize\n",
    "        Q_to = q + 0.5 * res\n",
    "\n",
    "        # Q_to=normalize(q, p=1.0, dim = 1)+0.5*normalize(res, p=1.0, dim = 1)\n",
    "        # Q_to=q+torch.sigmoid(self.dropout(self.encode1(res)))\n",
    "        Q_to = normalize(Q_to, p=1, dim=1)\n",
    "\n",
    "        ebs = node_embeddings.weight\n",
    "        ebs_c = community_embeddings\n",
    "        return z, q, Q_to, prior, recon_c, q_vg, ebs, ebs_c\n",
    "\n",
    "    def get_Q(self, z):\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q = q.pow((self.v + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e8ffe1-91d8-49c9-99f1-6e4d8166b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n",
    "\n",
    "def load_data(dataset):  # Planetoid to ours\n",
    "    adj=dataset.adj_label.numpy()\n",
    "\n",
    "    adj_=adj-np.eye(adj.shape[0])\n",
    "\n",
    "    label=dataset.y.numpy()\n",
    "    G=nx.from_numpy_array(adj_)\n",
    "    membership=[label[i] for i in range(G.number_of_nodes())]\n",
    "    return G,nx.adjacency_matrix(G), membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c245867-9434-4586-ae57-a0a4604eaf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_v(recon_c, q_y, prior, c, norm=None, pos_weight=None):\n",
    "    BCE = F.cross_entropy(recon_c, c, reduction='sum') / c.shape[0]\n",
    "    # BCE = F.binary_cross_entropy_with_logits(recon_c, c, pos_weight=pos_weight)\n",
    "    # return BCE\n",
    "\n",
    "    log_qy = torch.log(q_y  + 1e-20)\n",
    "    KLD = torch.sum(q_y*(log_qy - torch.log(prior)),dim=-1).mean()\n",
    "\n",
    "    ent = (- torch.log(q_y) * q_y).sum(dim=-1).mean()\n",
    "    # return BCE\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def get_assignment(G, device,model, dataset,num_classes=5, tpe=0):\n",
    "    model.eval()\n",
    "    edges = [(u,v) for u,v in G.edges()]\n",
    "    batch = torch.LongTensor(edges).to(device)  # （5278,2）\n",
    "\n",
    "    _, _,_,_,_,q, _,_ = model(dataset,batch[:, 0], batch[:, 1], 1.)\n",
    "\n",
    "    num_classes = q.shape[1]\n",
    "    q_argmax = q.argmax(dim=-1)\n",
    "\n",
    "    assignment = {}\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "\n",
    "    res = np.zeros((n_nodes, num_classes))\n",
    "    for idx, e in enumerate(edges):\n",
    "        if tpe == 0:\n",
    "            res[e[0], :] += q[idx, :].cpu().data.numpy()\n",
    "            res[e[1], :] += q[idx, :].cpu().data.numpy()\n",
    "        else:\n",
    "            res[e[0], q_argmax[idx]] += 1\n",
    "            res[e[1], q_argmax[idx]] += 1\n",
    "\n",
    "    res = res.argmax(axis=-1)\n",
    "    assignment = {i : res[i] for i in range(res.shape[0])}\n",
    "    return res, assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5bd3790-b065-4278-a14a-4390dbbd676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(dataset, device):\n",
    "\n",
    "    ANNEAL_RATE = 0.00003\n",
    "    temp_min = 0.3\n",
    "    temp = 1.\n",
    "\n",
    "    model = DICN(device, args.dropout, edge_num=args.edge_size, embedding_vg=args.embedding_vg,\n",
    "                  num_features=args.input_dim, node_num=args.node_num,\n",
    "                  embedding_size=args.embedding_size, alpha=args.alpha, num_clusters=args.n_clusters).to(device)\n",
    "\n",
    "    dataset_lt = data_preprocessing(dataset)\n",
    "\n",
    "    data1 = torch.Tensor(dataset_lt.x).to(device)\n",
    "    data = np.load('./pretrain/EV22NSO8_cora_dotproducted_vectors_128.npy')\n",
    "    z = torch.Tensor(data)\n",
    "    y = dataset_lt.y.cpu().numpy()\n",
    "\n",
    "    G, adj, gt_membership = load_data(dataset_lt)\n",
    "\n",
    "    train_edges = [(u, v) for u, v in G.edges()]\n",
    "    args.train_edges = train_edges\n",
    "    batch = torch.LongTensor(train_edges)\n",
    "    assert batch.shape == (len(train_edges), 2)\n",
    "\n",
    "    w = torch.cat((batch[:, 0], batch[:, 1])).to(device)\n",
    "    c = torch.cat((batch[:, 1], batch[:, 0])).to(device)\n",
    "\n",
    "    categorical_dim = len(set(gt_membership))\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    dataset = dataset.to(device)\n",
    "    dataset_lt = dataset_lt.to(device)\n",
    "\n",
    "    # get kmeans and pretrain cluster result\n",
    "    kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "    y_pred = kmeans.fit_predict(z.data.cpu().numpy())\n",
    "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "    acc, nmi, ari, f1 = eva(y, y_pred, 'pretrain')\n",
    "\n",
    "    model.vGraph.node_embeddings.weight = Parameter(z.to(device))\n",
    "\n",
    "    model.vGraph.community_embeddings.weight = Parameter(model.cluster_layer)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    z_to = z\n",
    "    best_acc, best_nmi, best_modu = 0., 0., 0.\n",
    "    nmi_acc, nmi_ari, nmi_f1 = 0., 0., 0.\n",
    "    best_acc_epoch, best_nmi_epoch, best_modu_epoch = 0, 0, 0\n",
    "    nmi_lst = []\n",
    "\n",
    "    for epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        if epoch % args.update_interval == 0:\n",
    "            z, Q, Q_to, prior, recon_c, q_vg, z_v, z_c = model(dataset_lt, w, c, temp)\n",
    "            # p = target_distribution(Q.detach())\n",
    "            p_to = target_distribution(Q_to.detach())\n",
    "            Q_v = model.get_Q(z_v)\n",
    "        else:\n",
    "            z, Q, Q_to, prior, recon_c, q_vg, z_v, z_c = model(dataset_lt, w, c, temp)\n",
    "            # p_to = target_distribution(Q_to.detach())\n",
    "            Q_v = model.get_Q(z_v)\n",
    "\n",
    "        # vGraph loss\n",
    "        res = torch.zeros([n_nodes, categorical_dim], dtype=torch.float32).to(device)\n",
    "        for idx, e in enumerate(train_edges):  #\n",
    "            res[e[0], :] += q_vg[idx, :]\n",
    "            res[e[1], :] += q_vg[idx, :]\n",
    "\n",
    "        loss_v = loss_function_v(recon_c, q_vg, prior, c.to(device), None, None)\n",
    "\n",
    "        # Clustering loss\n",
    "        kl_loss = F.kl_div(Q_to.log(), p_to, reduction='batchmean')\n",
    "\n",
    "        q_to = Q_to.detach().data.cpu().numpy().argmax(1)\n",
    "\n",
    "        q_z = Q.detach().data.cpu().numpy().argmax(1)\n",
    "        q_vg = res.detach().data.cpu().numpy().argmax(1)\n",
    "\n",
    "        loss_gat = SuperGAT.mix_supervised_attention_loss_with_pretraining(\n",
    "            loss=kl_loss * 100,\n",
    "            model=model.gat,\n",
    "            mixing_weight=sgat_args.att_lambda,\n",
    "            criterion=sgat_args.super_gat_criterion,\n",
    "            current_epoch=epoch,\n",
    "            pretraining_epoch=sgat_args.total_pretraining_epoch,\n",
    "        )\n",
    "\n",
    "        trade_off_loss = F.mse_loss(z_v, z)\n",
    "\n",
    "        loss = loss_gat + loss_v + 200 * trade_off_loss\n",
    "\n",
    "        acc, nmi, ari, f1 = eva(y, q_to, epoch)\n",
    "        nmi_lst.append(nmi)\n",
    "\n",
    "        acc_z, nmi_z, ari_z, f1_z = eva(y, q_z, epoch)\n",
    "\n",
    "        acc_vg, nmi_vg, ari_vg, f1_vg = eva(y, q_vg, epoch)\n",
    "\n",
    "        if nmi > best_nmi:\n",
    "            best_nmi = nmi\n",
    "            best_nmi_epoch = epoch\n",
    "            nmi_acc, nmi_ari, nmi_f1 = acc, ari, f1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * epoch), temp_min)\n",
    "\n",
    "\n",
    "    max_nmi = np.max(nmi_lst)\n",
    "    print(nmi_lst.index(max_nmi), max_nmi)\n",
    "\n",
    "    print(f\"the epoch of the best nmi:{best_nmi_epoch}and other indicators:{nmi_acc},nmi:{best_nmi},ari:{nmi_ari},f1:{nmi_f1}\")\n",
    "    return acc, nmi, ari, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd4ae4e-8288-4d7f-8d90-b01ea93ef1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255660e5-b066-4e00-8c3a-429d802fd917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda: True\n",
      "\u001b[32mNow loading dataset: Planetoid / Cora\u001b[0m\n",
      "Namespace(alpha=0.2, cuda=True, dataset_str='cora', dropout=0.0, dv=0.5, edge_size=10556, embedding_size=128, embedding_vg=128, gpu_id='0', hidden_size=256, input_dim=1433, k=None, lamda=0.1, lamda2=10.0, lr=0.001, max_d_epoch=100, max_epoch=201, n_clusters=7, name='Cora', node_num=2708, update_interval=5, weight_decay=0.005)\n",
      "SuperGATNet(\n",
      "  (conv1): SuperGAT(1433, 128, heads=8, concat=True, att_type=prob_mask_only, nsr=0.5, pnr=0.0)\n",
      "  (conv2): SuperGAT(1024, 128, heads=8, concat=False, att_type=prob_mask_only, nsr=0.5, pnr=0.0)\n",
      ")\n",
      "epoch pretrain:acc 0.4878, nmi 0.2928, ari 0.1813, f1 0.4529\n",
      "epoch 0:acc 0.3006, nmi 0.0060, ari -0.0002, f1 0.0757\n",
      "epoch 0:acc 0.3013, nmi 0.0046, ari -0.0004, f1 0.0727\n",
      "epoch 0:acc 0.2707, nmi 0.0246, ari 0.0332, f1 0.1815\n",
      "epoch 1:acc 0.3013, nmi 0.0073, ari -0.0002, f1 0.0771\n",
      "epoch 1:acc 0.3006, nmi 0.0060, ari -0.0007, f1 0.0717\n",
      "epoch 1:acc 0.2733, nmi 0.0255, ari 0.0352, f1 0.1834\n",
      "epoch 2:acc 0.3168, nmi 0.0316, ari 0.0057, f1 0.1080\n",
      "epoch 2:acc 0.3113, nmi 0.0197, ari 0.0029, f1 0.0927\n",
      "epoch 2:acc 0.2740, nmi 0.0254, ari 0.0353, f1 0.1833\n",
      "epoch 3:acc 0.3364, nmi 0.0641, ari 0.0131, f1 0.1467\n",
      "epoch 3:acc 0.3275, nmi 0.0515, ari 0.0083, f1 0.1276\n",
      "epoch 3:acc 0.2736, nmi 0.0250, ari 0.0349, f1 0.1817\n",
      "epoch 4:acc 0.3534, nmi 0.1009, ari 0.0190, f1 0.1799\n",
      "epoch 4:acc 0.3475, nmi 0.0910, ari 0.0152, f1 0.1782\n",
      "epoch 4:acc 0.2733, nmi 0.0244, ari 0.0359, f1 0.1800\n",
      "epoch 5:acc 0.3578, nmi 0.1116, ari 0.0234, f1 0.2012\n",
      "epoch 5:acc 0.3471, nmi 0.0962, ari 0.0146, f1 0.1827\n",
      "epoch 5:acc 0.2733, nmi 0.0243, ari 0.0363, f1 0.1802\n",
      "epoch 6:acc 0.3959, nmi 0.1727, ari 0.0482, f1 0.2689\n",
      "epoch 6:acc 0.3892, nmi 0.1642, ari 0.0383, f1 0.2588\n",
      "epoch 6:acc 0.2744, nmi 0.0246, ari 0.0371, f1 0.1800\n",
      "epoch 7:acc 0.4372, nmi 0.2340, ari 0.0869, f1 0.3318\n",
      "epoch 7:acc 0.4376, nmi 0.2331, ari 0.0843, f1 0.3274\n",
      "epoch 7:acc 0.2755, nmi 0.0248, ari 0.0367, f1 0.1820\n",
      "epoch 8:acc 0.4332, nmi 0.2523, ari 0.0984, f1 0.3450\n",
      "epoch 8:acc 0.4313, nmi 0.2486, ari 0.0932, f1 0.3398\n",
      "epoch 8:acc 0.2762, nmi 0.0243, ari 0.0365, f1 0.1819\n",
      "epoch 9:acc 0.5030, nmi 0.3140, ari 0.1665, f1 0.4315\n",
      "epoch 9:acc 0.5048, nmi 0.3185, ari 0.1647, f1 0.4325\n",
      "epoch 9:acc 0.2762, nmi 0.0235, ari 0.0352, f1 0.1815\n",
      "epoch 10:acc 0.4583, nmi 0.3000, ari 0.1548, f1 0.3961\n",
      "epoch 10:acc 0.4668, nmi 0.3074, ari 0.1593, f1 0.4046\n",
      "epoch 10:acc 0.2781, nmi 0.0240, ari 0.0362, f1 0.1809\n",
      "epoch 11:acc 0.5181, nmi 0.3648, ari 0.2235, f1 0.4660\n",
      "epoch 11:acc 0.5222, nmi 0.3703, ari 0.2250, f1 0.4683\n",
      "epoch 11:acc 0.2795, nmi 0.0242, ari 0.0374, f1 0.1834\n",
      "epoch 12:acc 0.5074, nmi 0.3841, ari 0.2573, f1 0.4670\n",
      "epoch 12:acc 0.5089, nmi 0.3865, ari 0.2554, f1 0.4689\n",
      "epoch 12:acc 0.2792, nmi 0.0247, ari 0.0380, f1 0.1819\n",
      "epoch 13:acc 0.4989, nmi 0.3809, ari 0.2468, f1 0.4766\n",
      "epoch 13:acc 0.5041, nmi 0.3838, ari 0.2509, f1 0.4802\n",
      "epoch 13:acc 0.2814, nmi 0.0253, ari 0.0396, f1 0.1804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1517/1286142779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0msgat_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mari\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mari\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1517/2678085917.py\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(dataset, device)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/my-env/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/my-env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Random model\n",
    "    setup_seed(12345)\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='train',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--name', type=str, default='Cora')\n",
    "    parser.add_argument('--max_epoch', type=int, default=201)\n",
    "    parser.add_argument('--max_d_epoch', type=int, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=0.005)\n",
    "    parser.add_argument('--dv', type=float, default=0.5)\n",
    "    parser.add_argument('--n_clusters', default=6, type=int)\n",
    "    parser.add_argument('--update_interval', default=5, type=int)  # [1,3,5]\n",
    "    parser.add_argument('--hidden_size', default=256, type=int)\n",
    "    parser.add_argument('--embedding_size', default=128, type=int)\n",
    "    parser.add_argument('--weight_decay', type=int, default=5e-3)\n",
    "    parser.add_argument('--embedding_vg', type=int, default=128)\n",
    "    parser.add_argument('--gpu_id', type=str, default='0')\n",
    "    parser.add_argument('--lamda', type=float, default=0.1)\n",
    "    parser.add_argument('--lamda2', type=float, default=10.0)\n",
    "    parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "    parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')\n",
    "    parser.add_argument('--dataset-str', type=str, default='cora', help='type of dataset.')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "    print(\"use cuda: {}\".format(args.cuda))\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    datasets = get_dataset(args.name)\n",
    "    dataset = datasets[0]\n",
    "    args.edge_size = dataset.num_edges\n",
    "    args.node_num=dataset.num_nodes\n",
    "    if args.name == 'Citeseer':\n",
    "        args.lr = 0.0001\n",
    "        args.k = None\n",
    "        args.n_clusters = 6\n",
    "    elif args.name == 'Cora':\n",
    "        args.lr = 0.0001\n",
    "        args.k = None\n",
    "        args.n_clusters = 7\n",
    "    elif args.name == \"Pubmed\":\n",
    "        args.lr = 0.001\n",
    "        args.k = None\n",
    "        args.n_clusters = 3\n",
    "    else:\n",
    "        args.k = None\n",
    "\n",
    "    args.input_dim = dataset.num_features\n",
    "\n",
    "    # the neighborhood parameters\n",
    "    sgat_args = get_args(model_name=\"GAT\",\n",
    "                         dataset_class=\"Planetoid\",\n",
    "                         dataset_name=\"Cora\",\n",
    "                         custom_key=\"EV13NSO8\")\n",
    "    sgat_args.num_hidden_features=128\n",
    "    sgat_args.outsize = 128\n",
    "    dataset_kwargs = {}\n",
    "    if sgat_args.dataset_class == \"ENSPlanetoid\":\n",
    "        dataset_kwargs[\"neg_sample_ratio\"] = sgat_args.neg_sample_ratio\n",
    "    if sgat_args.dataset_class == \"WikiCS\":\n",
    "        dataset_kwargs[\"split\"] = sgat_args.seed % 20\n",
    "    train_d, val_d, test_d = get_dataset_or_loader(\n",
    "        sgat_args.dataset_class, sgat_args.dataset_name, sgat_args.data_root,\n",
    "        batch_size=sgat_args.batch_size, seed=sgat_args.seed, num_splits=sgat_args.data_num_splits,\n",
    "        **dataset_kwargs,\n",
    "    )\n",
    "\n",
    "    args.lr=0.001\n",
    "\n",
    "    sgat_args.att_lambda=1\n",
    "    print(args)\n",
    "    acc, nmi, ari, f1 = trainer(dataset,device)\n",
    "    print(acc, nmi, ari, f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
