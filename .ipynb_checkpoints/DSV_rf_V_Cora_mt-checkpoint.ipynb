{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b593fadc-d9f7-4102-952f-eaf83fa5b440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my-env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "import networkx as nx\n",
    "from model import GAT\n",
    "\n",
    "from SuperGAT.model import SuperGATNet\n",
    "from SuperGAT.arguments import get_args\n",
    "from SuperGAT.data import getattr_d, get_dataset_or_loader\n",
    "from SuperGAT.layer import SuperGAT\n",
    "from SuperGAT.layer_cgat import CGATConv\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "import utils\n",
    "from utils import data_preprocessing, get_dataset\n",
    "from model_2 import GCNModelGumbel\n",
    "from evaluation import eva\n",
    "import community\n",
    "# 采用官方的vGraph联合D_S优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80c6151-9762-42a2-ba9b-bc04abf194b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAEGC(nn.Module):\n",
    "    def __init__(self, device,dropout,edge_num,embedding_vg,num_features, node_num, embedding_size, alpha, num_clusters, v=1):\n",
    "        super(DAEGC, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.v = v\n",
    "        self.node_num=node_num\n",
    "        self.gat = SuperGATNet(args=sgat_args, dataset_or_loader=train_d)\n",
    "    \n",
    "        self.vGraph=GCNModelGumbel(node_num,embedding_vg,num_clusters,dropout,device)\n",
    "        # self.encode1=nn.Linear(num_clusters,num_clusters) # (7,7)\n",
    "        # self.dropout=nn.Dropout(0.5)\n",
    "        # self.encode3=nn.Linear(embedding_size,embedding_size)  # (128,128)\n",
    "        \n",
    "        # cluster layer\n",
    "        self.cluster_layer = Parameter(torch.Tensor(num_clusters, embedding_size))  # （7,128）\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def pre_forward(self,dataset):\n",
    "        z = self.gat(dataset.x, dataset.edge_index,\n",
    "                     batch=getattr(dataset, \"batch\", None),\n",
    "                     attention_edge_index=getattr(dataset, \"train_edge_index\", None)) \n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, dataset,w,c,temp):\n",
    "    \n",
    "        z = self.gat(dataset.x, dataset.edge_index,\n",
    "                     batch=getattr(dataset, \"batch\", None),\n",
    "                     attention_edge_index=getattr(dataset, \"train_edge_index\", None))   # 邻域级表示\n",
    "        q = self.get_Q(z)  # 生成邻域级分布\n",
    "        \n",
    "        recon_c, q_vg,prior, node_embeddings,community_embeddings = self.vGraph(w, c, temp)  # 社区级表示\n",
    "        \n",
    "        res = torch.zeros([args.node_num, args.n_clusters], dtype=torch.float32).to(device) \n",
    "        for idx, e in enumerate(args.train_edges):  # 生成社区级分布\n",
    "            res[e[0], :] += q_vg[idx, :]\n",
    "            res[e[1], :] += q_vg[idx, :]\n",
    "        from torch.nn.functional import normalize\n",
    "        Q_to=q+0.5*res  # 融合两部分分布\n",
    "        \n",
    "        # Q_to=normalize(q, p=1.0, dim = 1)+0.5*normalize(res, p=1.0, dim = 1)\n",
    "        # Q_to=q+torch.sigmoid(self.dropout(self.encode1(res)))\n",
    "        Q_to = normalize(Q_to, p=1, dim = 1)  #  torch.Size([2708, 7])\n",
    "        \n",
    "        ebs=node_embeddings.weight  # 社区编码权重作为社区级表示\n",
    "        ebs_c=community_embeddings\n",
    "        return z,q,Q_to,prior, recon_c, q_vg, ebs,ebs_c\n",
    "\n",
    "    def get_Q(self, z):\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q = q.pow((self.v + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbf6818-ed15-42d2-bd01-22575266bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n",
    "\n",
    "def get_parameters(model):\n",
    "    params = []\n",
    "    for name, param in model.named_parameters():\n",
    "\n",
    "        if 'vGraph' not in name:\n",
    "            params.append(param)\n",
    "            print(name,param.shape)\n",
    "    print('此优化器返回的参数：',len(params))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91859a3-f729-40f4-902f-7c29a4773937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(args, epochs, nmi, modularity):\n",
    "    with open(args.log_file, 'a+') as f:\n",
    "        f.write('{},{},{},{},{},{},{},{}\\n'.format('vGraph',\n",
    "            args.dataset_str,\n",
    "            args.lr,\n",
    "            args.embedding_vg, args.lamda, epochs, nmi, modularity))\n",
    "\n",
    "\n",
    "def load_cora_citeseer(ds):  # ds:'cora'\n",
    "    dirpath = './data/{}'.format(ds)  # './data/cora'\n",
    "    edge_path = dirpath + '/{}.cites'.format(ds)  # 连边信息\n",
    "    content_path = dirpath + '/{}.content'.format(ds)  # 点内容信息\n",
    "\n",
    "    with open(content_path, 'r') as f:\n",
    "        content = f.readlines()  # 列表，2708个元素\n",
    "\n",
    "    mapping = {}  # 顶点重新编号\n",
    "    label_mapping = {}  # 标签编索引\n",
    "    membership = {}  # 将新顶点编号与标签索引对应上\n",
    "    for line in content:\n",
    "        tmp = line.strip().split('\\t')\n",
    "        mapping[tmp[0]] = len(mapping)  # 节点重新编号\n",
    "        try:\n",
    "            lab = label_mapping[tmp[-1]]\n",
    "        except:\n",
    "            label_mapping[tmp[-1]] = len(label_mapping)\n",
    "            lab = label_mapping[tmp[-1]]\n",
    "\n",
    "        membership[mapping[tmp[0]]] = lab\n",
    "    assert len(membership) == len(mapping)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    with open(edge_path, 'r') as f:\n",
    "        for line in f:\n",
    "            e0, e1 = line.strip().split('\\t')  # 分别是一条边的两个顶点，但是原顶点编号\n",
    "            try:\n",
    "                e0 = mapping[e0]  # 转为新顶点编号\n",
    "                e1 = mapping[e1]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            G.add_edge(e0, e1)\n",
    "\n",
    "    assert len(mapping) == G.number_of_nodes()  # 确保按边构建的图的顶点数也是2708\n",
    "\n",
    "    assert max(list(G.nodes())) == G.number_of_nodes() - 1  # 2707=2708-1\n",
    "    membership = [membership[i] for i in range(G.number_of_nodes())]  # 图G顶点对应的标签索引\n",
    "    return G, nx.adjacency_matrix(G), membership\n",
    "\n",
    "\n",
    "def load_data(dataset):  # Planetoid方法导入的数据集转化为社区级格式数据\n",
    "    adj=dataset.adj_label.numpy()  \n",
    "    print(\"邻接矩阵值为1的数目：\",np.sum(np.diagonal(adj)))  # 2708\n",
    "    adj_=adj-np.eye(adj.shape[0])  # 去掉对角线1\n",
    "    print(\"当减去对角线1时，对角线值是否为0\",adj_[10][10])\n",
    "    label=dataset.y.numpy()\n",
    "    G=nx.from_numpy_array(adj_)\n",
    "    membership=[label[i] for i in range(G.number_of_nodes())]   \n",
    "    return G,nx.adjacency_matrix(G), membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1104982-50d3-4a0e-978e-c9195a83fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_v(recon_c, q_y, prior, c, norm=None, pos_weight=None):  # 官方的损失函数\n",
    "    BCE = F.cross_entropy(recon_c, c, reduction='sum') / c.shape[0]\n",
    "    # BCE = F.binary_cross_entropy_with_logits(recon_c, c, pos_weight=pos_weight)\n",
    "    # return BCE\n",
    "\n",
    "    log_qy = torch.log(q_y  + 1e-20)\n",
    "    KLD = torch.sum(q_y*(log_qy - torch.log(prior)),dim=-1).mean()\n",
    "\n",
    "    ent = (- torch.log(q_y) * q_y).sum(dim=-1).mean()\n",
    "    return BCE\n",
    "    # return BCE + KLD\n",
    "\n",
    "def get_assignment(G, device,model, dataset,num_classes=5, tpe=0):  #\n",
    "    model.eval()\n",
    "    edges = [(u,v) for u,v in G.edges()]\n",
    "    batch = torch.LongTensor(edges).to(device)  # （5278,2）\n",
    "    # print(\"batch是否在cuda上，模型是否在：\",batch.device,next(model.parameters()).device)，batch不在\n",
    "    _, _,_,_,_,q, _,_ = model(dataset,batch[:, 0], batch[:, 1], 1.)  # q:（5278,7）\n",
    "\n",
    "    num_classes = q.shape[1]\n",
    "    q_argmax = q.argmax(dim=-1)\n",
    "\n",
    "    assignment = {}\n",
    "\n",
    "    n_nodes = G.number_of_nodes()\n",
    "\n",
    "    res = np.zeros((n_nodes, num_classes))  # （2708,7）\n",
    "    for idx, e in enumerate(edges):\n",
    "        if tpe == 0:\n",
    "            res[e[0], :] += q[idx, :].cpu().data.numpy()\n",
    "            res[e[1], :] += q[idx, :].cpu().data.numpy()\n",
    "        else:\n",
    "            res[e[0], q_argmax[idx]] += 1\n",
    "            res[e[1], q_argmax[idx]] += 1\n",
    "\n",
    "    res = res.argmax(axis=-1)  # 预测标签\n",
    "    assignment = {i : res[i] for i in range(res.shape[0])}\n",
    "    return res, assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c4b964-bd1c-49aa-8a46-41417d20ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nonoverlap_nmi(pred_membership, gt_membership):  # 官方的计算nmi指标\n",
    "    from clusim.clustering import Clustering\n",
    "    import clusim.sim as sim\n",
    "\n",
    "    pred = Clustering()\n",
    "    pred.from_membership_list(pred_membership)\n",
    "\n",
    "    gt= Clustering()\n",
    "    gt.from_membership_list(gt_membership)\n",
    "\n",
    "    ret = sim.nmi(pred, gt, norm_type='sum')\n",
    "    return ret\n",
    "\n",
    "def classical_modularity_calculator(graph, embedding, model='gcn_vae', cluster_number=5):\n",
    "    \"\"\"\n",
    "    Function to calculate the DeepWalk cluster centers and assignments.\n",
    "    \"\"\"\n",
    "    if model == 'gcn_vae':\n",
    "        assignments = embedding\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=cluster_number, random_state=0, n_init = 1).fit(embedding)\n",
    "        assignments = {i: int(kmeans.labels_[i]) for i in range(0, embedding.shape[0])}\n",
    "\n",
    "    modularity = community.modularity(assignments, graph)  # 计算模块度，1是字典，键为节点，值为对应社区，2是图。\n",
    "    return modularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e629d9-71b2-4194-829c-a29ca5c7a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(dataset,device):\n",
    "    # 部分参数\n",
    "    ANNEAL_RATE=0.00003\n",
    "    temp_min = 0.3\n",
    "    temp = 1.\n",
    "    # 模型架构\n",
    "    model = DAEGC(device,args.dropout,edge_num=args.edge_size,embedding_vg=args.embedding_vg,num_features=args.input_dim, node_num=args.node_num,\n",
    "                  embedding_size=args.embedding_size, alpha=args.alpha,num_clusters=args.n_clusters).to(device)\n",
    "\n",
    "    # 数据处理\n",
    "    dataset_lt = data_preprocessing(dataset)\n",
    "\n",
    "    # 数据的标签\n",
    "    data1 = torch.Tensor(dataset_lt.x).to(device)\n",
    "    data=np.load('./pretrain/EV22NSO8_cora_dotproducted_vectors_128.npy')  # 预训练节点嵌入，用于生成初始聚类中心\n",
    "    z = torch.Tensor(data)\n",
    "    y = dataset_lt.y.cpu().numpy()\n",
    "\n",
    "    G, adj, gt_membership = load_data(dataset_lt)  # 这里是用ds中dataset作为vgarph传入的数据G\n",
    "    \n",
    "    train_edges = [(u, v) for u, v in G.edges()]  # 边对\n",
    "    args.train_edges=train_edges\n",
    "    batch = torch.LongTensor(train_edges) \n",
    "    assert batch.shape == (len(train_edges), 2)\n",
    "\n",
    "    # 这样可以让w和c对应位置即为一条边的两个顶点索引\n",
    "    w = torch.cat((batch[:, 0], batch[:, 1])).to(device)  \n",
    "    c = torch.cat((batch[:, 1], batch[:, 0])).to(device)\n",
    "\n",
    "    categorical_dim = len(set(gt_membership))  # 类别数\n",
    "    n_nodes = G.number_of_nodes()  # 节点数\n",
    "    dataset=dataset.to(device)\n",
    "    dataset_lt=dataset_lt.to(device)\n",
    "    \n",
    "    # get kmeans and pretrain cluster result\n",
    "    kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "    y_pred = kmeans.fit_predict(z.data.cpu().numpy())\n",
    "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)  \n",
    "    acc, nmi, ari, f1 = eva(y, y_pred, 'pretrain')  \n",
    "\n",
    "    model.vGraph.node_embeddings.weight = Parameter(z.to(device))  # 将得到的嵌入输入到vg中的权重中\n",
    "    # print(\"聚类中心层的维度：\",model.cluster_layer.data.t().shape)\n",
    "    model.vGraph.community_embeddings.weight = Parameter(model.cluster_layer)\n",
    "    # print(\"社区嵌入的权重维度：\",model.vGraph.community_embeddings.weight.shape)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)  # 这里应该是全部参数一起优化\n",
    "    z_to=z\n",
    "    # print(\"模型是否在cuda上：\",next(model.parameters()).device)\n",
    "    best_acc,best_nmi,best_modu=0.,0.,0.\n",
    "    nmi_acc,nmi_ari,nmi_f1=0.,0.,0.\n",
    "    best_acc_epoch,best_nmi_epoch,best_modu_epoch=0,0,0\n",
    "    nmi_lst=[]  # 存储q_to得到的nmi\n",
    "    nmi_rst=[]  # 存储q_vg得到的nmi--4\n",
    "    mod_lst=[]\n",
    "    mod_rst=[]\n",
    "    \n",
    "    for epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "    \n",
    "        if epoch % args.update_interval ==0:  # z_to是将v中权重与s所学的z经过加权求和得到的综合结果，用它来计算kl损失\n",
    "            z, Q,Q_to,prior,recon_c,q_vg,z_v,z_c= model(dataset_lt,w,c,temp)  \n",
    "            # p = target_distribution(Q.detach())\n",
    "            p_to = target_distribution(Q_to.detach())\n",
    "            Q_v=model.get_Q(z_v)\n",
    "        else:\n",
    "            z,Q,Q_to,prior,recon_c,q_vg,z_v,z_c = model(dataset_lt,w,c,temp)\n",
    "            # p_to = target_distribution(Q_to.detach())\n",
    "            Q_v=model.get_Q(z_v)\n",
    "        print(\"Q_to的维度和类型：\",Q_to.shape,type(Q_to))\n",
    "       \n",
    "        # print(\"查看Q，Q_to，Q_v,q_vg的维度：\",Q.shape,Q_to.shape,Q_v.shape,q_vg.shape)  \n",
    "        \n",
    "        # 计算vGraph损失\n",
    "        res = torch.zeros([n_nodes, categorical_dim], dtype=torch.float32).to(device)  # (2708,7)\n",
    "        for idx, e in enumerate(train_edges):  #\n",
    "            res[e[0], :] += q_vg[idx, :]\n",
    "            res[e[1], :] += q_vg[idx, :]\n",
    "        \n",
    "        loss_v = loss_function_v(recon_c, q_vg, prior, c.to(device), None, None)\n",
    "        print(\"loss_v的数量级：\",loss_v)\n",
    "        \n",
    "        # 计算KL损失\n",
    "        kl_loss = F.kl_div(Q_to.log(), p_to, reduction='batchmean')  # 数量级：2.87*10-5\n",
    "        print('查看kl损失是否变化：', 100 * kl_loss)\n",
    "        \n",
    "        q_to = Q_to.detach().data.cpu().numpy().argmax(1)  # 这里原来是用Q，这里改为Q_to试,(2708,)\n",
    "        # print(\"q_to的维度：\",q_to.shape)\n",
    "        q_z = Q.detach().data.cpu().numpy().argmax(1)\n",
    "        q_vg=res.detach().data.cpu().numpy().argmax(1)\n",
    "\n",
    "        loss_gat = SuperGAT.mix_supervised_attention_loss_with_pretraining(  # 数量级：12.5336\n",
    "            loss=kl_loss*100,  # 这里需要乘上100么？\n",
    "            model=model.gat,  # 这里要注意，不是model\n",
    "            mixing_weight=sgat_args.att_lambda,\n",
    "            criterion=sgat_args.super_gat_criterion,\n",
    "            current_epoch=epoch,\n",
    "            pretraining_epoch=sgat_args.total_pretraining_epoch,  # 默认为0\n",
    "        )\n",
    "        print(\"loss_gat损失：\",loss_gat)\n",
    "       \n",
    "        trade_off_loss = F.mse_loss(z_v, z)\n",
    "        print(\"两部分节点嵌入的损失：\",200*trade_off_loss)\n",
    "        \n",
    "        # loss =100 * kl_loss+loss_gat+loss_v+trade_off_loss  # 后两者的数量级接近，因此可以不加系数，前面的kl损失数量级太小，酌情增加系数\n",
    "        loss =loss_gat+loss_v+200*trade_off_loss\n",
    "        \n",
    "        acc, nmi, ari, f1 = eva(y, q_to, epoch)  # 使用混合分布进行预测\n",
    "        nmi_lst.append(nmi)\n",
    "        \n",
    "        acc_z, nmi_z, ari_z, f1_z = eva(y, q_z, epoch)  # 单独使用邻域分布评测\n",
    "        \n",
    "        acc_vg, nmi_vg, ari_vg, f1_vg = eva(y, q_vg, epoch)  # vgraph节点分布评测\n",
    "        \n",
    "        nmi_rst.append(nmi_vg)\n",
    "        \n",
    "        if nmi>best_nmi:\n",
    "            best_nmi=nmi\n",
    "            best_nmi_epoch=epoch\n",
    "            nmi_acc,nmi_ari,nmi_f1=acc,ari,f1\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 1 == 0:   # 评测，用的是q_vg\n",
    "            if epoch % 100 == 0:  # 更新社区级部分参数\n",
    "                temp = np.maximum(temp * np.exp(-ANNEAL_RATE * epoch), temp_min)  # 是用于gumble采样中tau参数的，它越小，采的数据越接近独热\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            membership, assignment = get_assignment(G,device, model,dataset, categorical_dim)  # V的方法（res）生成预测标签\n",
    "            print(\"memebership的类型和维度：\",type(membership),membership.shape)  #  <class 'numpy.ndarray'> (2708,)\n",
    "\n",
    "            assignment_res = {i : q_vg[i] for i in range(q_vg.shape[0])}\n",
    "            modularity_res = classical_modularity_calculator(G,assignment_res)  # 直接由res产生的节点id：标签字典\n",
    "            \n",
    "            assignment_z = {i : q_z[i] for i in range(q_z.shape[0])}\n",
    "            modularity_z = classical_modularity_calculator(G,assignment_z) \n",
    "            assignment_to = {i : q_to[i] for i in range(q_to.shape[0])}\n",
    "            modularity_to = classical_modularity_calculator(G,assignment_to) \n",
    "            mod_lst.append(modularity_to)\n",
    "            \n",
    "            modularity = classical_modularity_calculator(G,assignment)  # V对应的计算模块度\n",
    "            nmi_vo = calc_nonoverlap_nmi(membership.tolist(), gt_membership)  # 计算nmi\n",
    "            nmi_rst.append(nmi_vo)\n",
    "            \n",
    "            nmi_res=calc_nonoverlap_nmi(q_vg.tolist(), gt_membership)\n",
    "            nmi_z=calc_nonoverlap_nmi(q_z.tolist(), gt_membership)\n",
    "            mod_rst.append(modularity)\n",
    "            \n",
    "            print(\"q_to对应的模块度\",'modularity_to：', modularity_to)\n",
    "            print(\"q_z对应的nmi和模块度：\",'Epoch：', epoch, 'nmi_z', nmi_z, 'modularity_z：', modularity_z)\n",
    "            print('Epoch：', epoch, 'nmi', nmi_vo, 'nmi_res', nmi_res,'modularity：', modularity,'modularity_res：', modularity_res)\n",
    "            logging(args, epoch, nmi, modularity)\n",
    "    print(\"检测输出模块度个数：\",len(mod_lst))\n",
    "    max_mod=np.max(mod_lst)\n",
    "    max_nmi=np.max(nmi_lst)\n",
    "    print(\"保存①最佳mod对应epoch及最佳mod：\",mod_lst.index(max_mod),max_mod)\n",
    "    print(\"保存①最佳nmi对应epoch及最佳nmi：\",nmi_lst.index(max_nmi),max_nmi)\n",
    "    \n",
    "    print(\"检测输出模块度个数：\",len(mod_rst))\n",
    "    max_modr=np.max(mod_rst)\n",
    "    max_nmir=np.max(nmi_rst)\n",
    "    print(\"保存⑤最佳mod对应epoch及最佳mod：\",mod_rst.index(max_modr),max_modr)\n",
    "    print(\"保存⑤最佳nmi对应epoch及最佳nmi：\",nmi_rst.index(max_nmir),max_nmir)\n",
    "    print(f\"最佳的nmi对应epoch{best_nmi_epoch}及指标acc:{nmi_acc},nmi:{best_nmi},ari:{nmi_ari},f1:{nmi_f1}\")\n",
    "    return acc, nmi, ari, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1739c2f0-82c8-4f9e-9577-2e78f81aca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812a1d7-3595-4675-ad7c-dc337f24ec1e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mNow loading dataset: Planetoid / Cora\u001b[0m\n",
      "sgat_args的参数： Namespace(att_lambda=1, attention_type='prob_mask_only', batch_size=128, checkpoint_dir='../checkpoints', custom_key='EV13NSO8', data_num_splits=1, data_root='~/graph-data', data_sampler=None, data_sampling_num_hops=None, data_sampling_size=None, dataset_class='Planetoid', dataset_name='Cora', dropout=0.6, early_stop_patience=-1, early_stop_queue_length=100, early_stop_threshold_loss=-1.0, early_stop_threshold_perf=-1.0, edge_sampling_ratio=0.8, epochs=500, gpu_deny_list=[1, 2, 3], heads=8, is_cgat_full=False, is_cgat_ssnc=False, is_link_gnn=False, is_super_gat=True, l1_lambda=0.0, l2_lambda=0.008228864972965771, link_lambda=0.0, loss=None, lr=0.005, m='', model_name='GAT', neg_sample_ratio=0.5, num_gpus_to_use=1, num_gpus_total=3, num_hidden_features=128, num_layers=2, out_heads=8, outsize=128, perf_task_for_val='Node', perf_type='accuracy', pool_name=None, pretraining_noise_ratio=0.0, save_model=False, save_plot=False, scaling_factor=None, seed=42, start_epoch=0, super_gat_criterion=None, task_type='Node_Transductive', to_undirected=False, to_undirected_at_neg=False, total_pretraining_epoch=0, use_bn=False, use_early_stop=False, use_pretraining=False, val_interval=1, verbose=2)\n",
      "args中的学习率和平滑系数： 0.001 0.1\n",
      "Namespace(alpha=0.2, cuda=True, dataset_str='cora', dropout=0.0, dv=0.5, edge_size=10556, embedding_size=128, embedding_vg=128, gpu_id='0', hidden_size=256, input_dim=1433, k=None, lamda=0.1, lamda2=10.0, log_file='nonoverlapping.log', lr=0.001, max_d_epoch=100, max_epoch=201, n_clusters=7, name='Cora', node_num=2708, update_interval=5, weight_decay=0.005)\n",
      "SuperGATNet(\n",
      "  (conv1): SuperGAT(1433, 128, heads=8, concat=True, att_type=prob_mask_only, nsr=0.5, pnr=0.0)\n",
      "  (conv2): SuperGAT(1024, 128, heads=8, concat=False, att_type=prob_mask_only, nsr=0.5, pnr=0.0)\n",
      ")\n",
      "邻接矩阵值为1的数目： 2708.0\n",
      "当减去对角线1时，对角线值是否为0 0.0\n",
      "epoch pretrain:acc 0.4878, nmi 0.2928, ari 0.1813, f1 0.4529\n",
      "Q_to的维度和类型： torch.Size([2708, 7]) <class 'torch.Tensor'>\n",
      "loss_v的数量级： tensor(7.9039, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "查看kl损失是否变化： tensor(0.0489, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "loss_gat损失： tensor(1.1159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "两部分节点嵌入的损失： tensor(3.7442, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "epoch 0:acc 0.3006, nmi 0.0060, ari -0.0002, f1 0.0757\n",
      "epoch 0:acc 0.3013, nmi 0.0046, ari -0.0004, f1 0.0727\n",
      "epoch 0:acc 0.2707, nmi 0.0246, ari 0.0332, f1 0.1815\n",
      "memebership的类型和维度： <class 'numpy.ndarray'> (2708,)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 设置随机数种子\n",
    "    setup_seed(12345)\n",
    "    # daegc的参数\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='train',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--name', type=str, default='Cora')\n",
    "    parser.add_argument('--max_epoch', type=int, default=201)\n",
    "    parser.add_argument('--max_d_epoch', type=int, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=0.005)  # 学习率大小\n",
    "    parser.add_argument('--dv', type=float, default=0.5)\n",
    "    parser.add_argument('--n_clusters', default=6, type=int)\n",
    "    parser.add_argument('--update_interval', default=5, type=int)  # [1,3,5]\n",
    "    parser.add_argument('--hidden_size', default=256, type=int)\n",
    "    parser.add_argument('--embedding_size', default=128, type=int)  # 默认16，要与传入的预训练张量第二维度一致\n",
    "    parser.add_argument('--weight_decay', type=int, default=5e-3)\n",
    "    parser.add_argument('--embedding_vg', type=int, default=128)\n",
    "    parser.add_argument('--gpu_id', type=str, default='0')\n",
    "    parser.add_argument('--lamda', type=float, default=0.1)  ## For the smoothness trick，vgraph的正则项系数，\n",
    "    parser.add_argument('--lamda2', type=float, default=10.0)  ## For the trade_off embeddings\n",
    "    parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "    parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')\n",
    "    parser.add_argument('--dataset-str', type=str, default='cora', help='type of dataset.')\n",
    "    parser.add_argument('--log-file', type=str, default='nonoverlapping.log', help='log path')\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "    print(\"use cuda: {}\".format(args.cuda))\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    datasets = get_dataset(args.name)\n",
    "    dataset = datasets[0]\n",
    "    args.edge_size = dataset.num_edges  # 10556\n",
    "    args.node_num=dataset.num_nodes\n",
    "    if args.name == 'Citeseer':\n",
    "        args.lr = 0.0001\n",
    "        args.k = None\n",
    "        args.n_clusters = 6\n",
    "    elif args.name == 'Cora':\n",
    "        args.lr = 0.0001\n",
    "        args.k = None\n",
    "        args.n_clusters = 7\n",
    "    elif args.name == \"Pubmed\":\n",
    "        args.lr = 0.001\n",
    "        args.k = None\n",
    "        args.n_clusters = 3\n",
    "    else:\n",
    "        args.k = None\n",
    "\n",
    "    args.input_dim = dataset.num_features\n",
    "\n",
    "    # supergat的参数\n",
    "    sgat_args = get_args(model_name=\"GAT\", \n",
    "                         dataset_class=\"Planetoid\",  \n",
    "                         dataset_name=\"Cora\",  \n",
    "                         custom_key=\"EV13NSO8\")  \n",
    "    sgat_args.num_hidden_features=128\n",
    "    sgat_args.outsize = 128\n",
    "    dataset_kwargs = {}\n",
    "    if sgat_args.dataset_class == \"ENSPlanetoid\":\n",
    "        dataset_kwargs[\"neg_sample_ratio\"] = sgat_args.neg_sample_ratio\n",
    "    if sgat_args.dataset_class == \"WikiCS\":\n",
    "        dataset_kwargs[\"split\"] = sgat_args.seed % 20\n",
    "    train_d, val_d, test_d = get_dataset_or_loader(\n",
    "        sgat_args.dataset_class, sgat_args.dataset_name, sgat_args.data_root,\n",
    "        batch_size=sgat_args.batch_size, seed=sgat_args.seed, num_splits=sgat_args.data_num_splits,\n",
    "        **dataset_kwargs,\n",
    "    )\n",
    "    args.lr=0.001\n",
    "    # args.lamda=0.01\n",
    "    sgat_args.att_lambda=1  \n",
    "    print(\"sgat_args的参数：\",sgat_args)\n",
    "    print(\"args中的学习率和平滑系数：\",args.lr,args.lamda)\n",
    "    print(args)\n",
    "    acc, nmi, ari, f1 = trainer(dataset,device)\n",
    "    print(acc, nmi, ari, f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
